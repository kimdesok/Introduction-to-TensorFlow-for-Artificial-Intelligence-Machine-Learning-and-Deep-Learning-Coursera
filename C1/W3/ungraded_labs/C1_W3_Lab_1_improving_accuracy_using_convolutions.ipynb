{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF0PtFcrh7DC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W3/ungraded_labs/C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "# Ungraded Lab: Improving Computer Vision Accuracy using Convolutions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6396DKnr-xp"
      },
      "source": [
        "# Shallow Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnCNAG-VecJ9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "training_images = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcsRtq9OLorS"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "# Setup training parameters\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(f'\\nMODEL TRAINING:')\n",
        "history = model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(f'\\nMODEL EVALUATION:')\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "\n",
        "\n",
        "Using a filter (like [this](https://en.wikipedia.org/wiki/Kernel_(image_processing))), \n",
        "\n",
        "\n",
        "\n",
        "Run the code below. This is the same neural network as earlier, but this time with [Convolution](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers added first. It will take longer, but look at the impact on the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0tFgT1MMKi6"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "modelc = tf.keras.models.Sequential([\n",
        "                                                         \n",
        "  # Add convolutions and max pooling\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "  # Add the same layers as before\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "modelc.summary()\n",
        "\n",
        "# Use same settings\n",
        "modelc.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(f'\\nMODEL TRAINING:')\n",
        "historyc = modelc.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "# Evaluate on the test set\n",
        "print(f'\\nMODEL EVALUATION:')\n",
        "test_loss = modelc.evaluate(test_images, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "fig.patch.set_facecolor('xkcd:mint green')\n",
        "plt.plot(history.history['loss'],label='Loss')\n",
        "plt.plot(historyc.history['loss'],label='cLoss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.patch.set_facecolor('xkcd:mint green')\n",
        "plt.plot(history.history['accuracy'], label ='Accuracy')\n",
        "plt.plot(historyc.history['accuracy'], label ='cAccuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "metadata": {
        "id": "w4AwliRpjXzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        " [Conv2D layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D). \n",
        "\n",
        "[MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) \n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA"
      },
      "source": [
        "Then you added another convolution and flattened the output.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "  tf.keras.layers.Flatten(),\n",
        "  \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX"
      },
      "source": [
        "After this, you'll just have the same DNN structure as the non convolutional version. The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load vgg model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zC_aZmAhBsoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize filter shapes\n",
        "print(' layer name      layer shape       filter shape')\n",
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# get filter weights\n",
        "\tfilters, biases = layer.get_weights()\n",
        "\tprint(layer.name, layer.output.shape, filters.shape)\n",
        " \n",
        " #An architectural concern with a convolutional neural network is that \n",
        " #the depth of a filter must match the depth of the input for the filter (e.g. the number of channels)"
      ],
      "metadata": {
        "id": "Mj5Z5k1EBswA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[1].get_weights()"
      ],
      "metadata": {
        "id": "LlPPIGjCIS1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)"
      ],
      "metadata": {
        "id": "B2622wwrIVjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# plot first few filters\n",
        "n_filters, ix = 6, 1\n",
        "for i in range(n_filters):\n",
        "\t# get the filter\n",
        "\tf = filters[:, :, :, i]\n",
        "\t# plot each channel separately\n",
        "\tfor j in range(3):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = plt.subplot(n_filters, 3, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tplt.imshow(f[:, :, j], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZwOwEoeSIfz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://machinelearningmastery.com/wp-content/uploads/2019/02/bird.jpg'"
      ],
      "metadata": {
        "id": "XHZ2tQ74KIdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot feature map of first conv layer for given image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# redefine model to output right after the first hidden layer\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "model.summary()\n",
        "# load the image with the required shape\n",
        "img = load_img('bird.jpg', target_size=(224, 224))\n",
        "# convert the image to an array\n",
        "img = img_to_array(img)\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot all 64 maps in an 8x8 squares\n",
        "square = 8\n",
        "ix = 1\n",
        "for _ in range(square):\n",
        "\tfor _ in range(square):\n",
        "\t\t# specify subplot and turn of axis\n",
        "\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\tax.set_xticks([])\n",
        "\t\tax.set_yticks([])\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tpyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "pyplot.show()\n",
        "\n",
        "#The result will be a feature map with 224x224x64. \n",
        "#We can plot all 64 two-dimensional images as an 8×8 square of images."
      ],
      "metadata": {
        "id": "dV-gv4GkMBCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize feature maps output from each block in the vgg model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "# load the model\n",
        "model = VGG16()\n",
        "# redefine model to output right after the first hidden layer\n",
        "ixs = [2, 5, 9, 13, 17]\n",
        "outputs = [model.layers[i].output for i in ixs]\n",
        "model = Model(inputs=model.inputs, outputs=outputs)\n",
        "# load the image with the required shape\n",
        "img = load_img('bird.jpg', target_size=(224, 224))\n",
        "# convert the image to an array\n",
        "img = img_to_array(img)\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# prepare the image (e.g. scale pixel values for the vgg)\n",
        "img = preprocess_input(img)\n",
        "# get feature map for first hidden layer\n",
        "feature_maps = model.predict(img)\n",
        "# plot the output from each block\n",
        "square = 8\n",
        "for fmap in feature_maps:\n",
        "\t# plot all 64 maps in an 8x8 squares\n",
        "\tix = 1\n",
        "\tfor _ in range(square):\n",
        "\t\tfor _ in range(square):\n",
        "\t\t\t# specify subplot and turn of axis\n",
        "\t\t\tax = pyplot.subplot(square, square, ix)\n",
        "\t\t\tax.set_xticks([])\n",
        "\t\t\tax.set_yticks([])\n",
        "\t\t\t# plot filter channel in grayscale\n",
        "\t\t\tpyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
        "\t\t\tix += 1\n",
        "\t# show the figure\n",
        "\tpyplot.show()"
      ],
      "metadata": {
        "id": "jmakQDmVOZEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "Let's explore how to show the convolutions graphically. The cell below prints the first 100 labels in the test set, and you can see that the ones at index `0`, index `23` and index `28` are all the same value (i.e. `9`). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the dense layer is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-6nX4QsOku6"
      },
      "outputs": [],
      "source": [
        "print(test_labels[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FGsHhv6JvDx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "\n",
        "f, axarr = plt.subplots(3,4)\n",
        "\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  print(test_images[FIRST_IMAGE].shape)\n",
        "  print(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1).shape)\n",
        "\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  print(f1.shape)\n",
        "\n",
        "  axarr[0,x].imshow(f1[0,:,:,CONVOLUTION_NUMBER], cmap='inferno')\n",
        "\n",
        "  axarr[0,x].grid(False)\n",
        "  \n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2.reshape(28, 28), cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  \n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3.reshape(28, 28), cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "### EXERCISES\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}